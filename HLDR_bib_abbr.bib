
@article{ounpraseuth_linear_2015,
	title = {Linear dimension reduction for multiple heteroscedastic multivariate normal populations},
	volume = {05},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	doi = {10.4236/ojs.2015.54033},
	abstract = {For the case where all multivariate normal parameters are known, we derive a new linear dimension reduction (LDR) method to determine a low-dimensional subspace that preserves or nearly preserves the original feature-space separation of the individual populations and the Bayes probability of misclassification. We also give necessary and sufficient conditions which provide the smallest reduced dimension that essentially retains the Bayes probability of misclassification from the original full-dimensional space in the reduced space. Moreover, our new LDR procedure requires no computationally expensive optimization procedure. Finally, for the case where parameters are unknown, we devise a LDR method based on our new theorem and compare our LDR method with three competing LDR methods using Monte Carlo simulations and a parametric bootstrap based on real data.},
	language = {en},
	number = {04},
	urldate = {2016-06-22},
	journal = {OJS},
	author = {Ounpraseuth, Songthip T. and Young, Phil D. and Van Zyl, Johanna S. and Nelson, Tyler W. and Young, Dean M.},
	month = may,
	year = {2015},
	pages = {311},
	file = {Full Text PDF:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\TH8XQ62W\\Ounpraseuth et al. - 2015 - Linear Dimension Reduction for Multiple Heterosced.pdf:application/pdf;Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\F9DG9S95\\PaperInformation.html:text/html}
}

@article{barker_partial_2003,
	title = {Partial least squares for discrimination},
	volume = {17},
	number = {3},
	urldate = {2016-06-22},
	journal = {J. Chemometr.},
	author = {Barker, Matthew and Rayens, William},
	year = {2003},
	pages = {166--173},
	file = {[PDF] from researchgate.net:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\XD387UQN\\Barker and Rayens - 2003 - Partial least squares for discrimination.pdf:application/pdf;Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\37SS86KN\\abstract\;jsessionid=B9F5094ED4C4AE249759F7B76E10E15B.html:text/html}
}

@book{bellman_adaptive_1961,
	title = {Adaptive {Control} {Processes}: {A} {Guided} {Tour}},
	isbn = {978-0-691-07901-1},
	shorttitle = {Adaptive {Control} {Processes}},
	abstract = {The book description for "Adaptive Control Processes" is currently unavailable.},
	urldate = {2016-06-22},
	publisher = {Princeton University Press},
	author = {Bellman, Richard},
	year = {1961}
}

@article{boulesteix_partial_2007,
	title = {Partial least squares: a versatile tool for the analysis of high-dimensional genomic data},
	volume = {8},
	issn = {1467-5463, 1477-4054},
	shorttitle = {Partial least squares},
	doi = {10.1093/bib/bbl016},
	language = {en},
	number = {1},
	urldate = {2016-06-22},
	journal = {Brief. Bioinform.},
	author = {Boulesteix, Anne-Laure and Strimmer, Korbinian},
	month = jan,
	year = {2007},
	pmid = {16772269},
	pages = {32--44},
	file = {Full Text PDF:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\4M6AAUTQ\\Boulesteix and Strimmer - 2007 - Partial least squares a versatile tool for the an.pdf:application/pdf;Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\HKIE7PQ2\\32.html:text/html}
}

@article{cook_sliced_1991,
	title = {Sliced inverse regression for dimension reduction: comment},
	volume = {86},
	issn = {0162-1459},
	shorttitle = {Sliced {Inverse} {Regression} for {Dimension} {Reduction}},
	doi = {10.2307/2290564},
	number = {414},
	urldate = {2016-06-22},
	journal = {J. Am. Stat. Assoc.},
	author = {Cook, R. Dennis and Weisberg, Sanford},
	year = {1991},
	pages = {328--332},
	file = {JSTOR Full Text PDF:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\V8BRVCMI\\Cook and Weisberg - 1991 - Sliced Inverse Regression for Dimension Reduction.pdf:application/pdf}
}

@article{cook_theory_2001,
	title = {Theory \& methods: {Special} {Invited} {Paper}: {Dimension} reduction and visualization in discriminant analysis},
	volume = {43},
	copyright = {Australian Statistical Publishing Association Inc 2001},
	issn = {1467-842X},
	shorttitle = {Theory \& {Methods}},
	doi = {10.1111/1467-842X.00164},
	abstract = {This paper discusses visualization methods for discriminant analysis. It does not address numerical methods for classification per se, but rather focuses on graphical methods that can be viewed as pre-processors, aiding the analyst's understanding of the data and the choice of a final classifier. The methods are adaptations of recent results in dimension reduction for regression, including sliced inverse regression and sliced average variance estimation. A permutation test is suggested as a means of determining dimension, and examples are given throughout the discussion.},
	language = {en},
	number = {2},
	urldate = {2016-06-22},
	journal = {Aust. N.Z. J. Stat.},
	author = {Cook, R. Dennis and Yin, Xiangrong},
	month = jun,
	year = {2001},
	keywords = {central subspaces, dimension reduction, regression, regression graphics, sliced average variance estimation (SAVE)., sliced inverse regression (SIR)},
	pages = {147--199},
	file = {Full Text PDF:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\ZR8AI58W\\Cook and Yin - 2001 - Theory & Methods Special Invited Paper Dimension.pdf:application/pdf;Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\GFF7FGU3\\abstract.html:text/html}
}

@article{fisher_use_1936,
	title = {The use of multiple measurements in taxonomic problems},
	volume = {7},
	copyright = {1936 Blackwell Publishing Ltd/University College London},
	issn = {2050-1439},
	doi = {10.1111/j.1469-1809.1936.tb02137.x},
	abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
	language = {en},
	number = {2},
	urldate = {2016-06-22},
	journal = {Ann. Eugenic.},
	author = {Fisher, R. A.},
	month = sep,
	year = {1936},
	pages = {179--188},
	file = {Full Text PDF:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\42K2NGWU\\Fisher - 1936 - The Use of Multiple Measurements in Taxonomic Prob.pdf:application/pdf;Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\3ATK79BV\\abstract.html:text/html}
}

@book{fukunaga_introduction_1990,
	address = {Boston},
	edition = {2nd Edition},
	title = {Introduction to {Statistical} {Pattern} {Recognition}},
	isbn = {978-0-12-269851-4},
	abstract = {This completely revised second edition presents an introduction to statistical pattern recognition.  Pattern recognition in general covers a wide range of problems: it is applied to engineering problems, such as character readers and wave form analysis as well as to brain modeling in biology and psychology.  Statistical decision and estimation, which are the main subjects of this book, are regarded as fundamental to the study of pattern recognition.  This book is appropriate as a text for introductory courses in pattern recognition and as a reference book for workers in the field.  Each chapter contains computer projects as well as exercises.},
	language = {English},
	publisher = {Academic Press},
	author = {Fukunaga, Keinosuke},
	month = oct,
	year = {1990}
}

@article{haff_estimation_1979,
	title = {Estimation of the inverse covariance matrix: random mixtures of the inverse wishart matrix and the identity},
	volume = {7},
	issn = {0090-5364},
	shorttitle = {Estimation of the {Inverse} {Covariance} {Matrix}},
	abstract = {Let Sp × p have a nonsingular Wishart distribution with unknown matrix Σ and k degrees of freedom. For two different loss functions, estimators of Σ-1 are given which dominate the obvious estimators \$aS{\textasciicircum}\{-1\}, 0 {\textless} a {\textbackslash}leqslant k - p - 1\$. Our class of estimators C includes random mixtures of S-1 and I. A subclass \${\textbackslash}mathscr\{C\}\_0 {\textbackslash}subset {\textbackslash}mathscr\{C\}\$ was given by Haff. Here, we show that any member of C0 is dominated in C. Some troublesome aspects of the estimation problem are discussed, and the theory is supplemented by simulation results.},
	number = {6},
	urldate = {2016-06-22},
	journal = {Ann. Stat.},
	author = {Haff, L. R.},
	year = {1979},
	pages = {1264--1276}
}

@article{hennig_asymmetric_2004,
	title = {Asymmetric linear dimension reduction for classification},
	volume = {13},
	issn = {1061-8600},
	doi = {10.1198/106186004X12740},
	number = {4},
	urldate = {2016-06-22},
	journal = {J. Comput. Graph. Stat.},
	author = {Hennig, Christian},
	month = dec,
	year = {2004},
	pages = {930--945},
	file = {Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\QXTXSXJ9\\106186004X12740.html:text/html}
}

@inproceedings{kumar_generalization_1996,
	title = {A generalization of linear discriminant analysis in maximum likelihood framework},
	abstract = {The Fisher--Rao linear discriminant analysis (LDA) is a valuable tool for multi-class classification and data reduction. We investigate LDA within the maximum likelihood framework and propose a general formulation to handle heteroscedasticity. Small size numerical experiments with randomly generated data verify the validity of our formulation. I. Introduction  Linear discriminant analysis (LDA) is a mathematical tool widely used for dimensionality reduction and multi-class classification [1], [2], [3]. Our interest in LDA [4] stems from our desire to use auditory features in speech recognition and from the encouraging results obtained by Brown [5]. However, inconsistent modeling assumptions between LDA, and the models used for recognition yield final systems with non-optimum performance. LDA can be derived as a maximum likelihood method for normal populations with different means, and common co-variance matrices [6]. Hastie [7] has further generalized this approach to the case where cl...},
	author = {Kumar, Nagendra and Andreou, Andreas G. and Andreou, Nagendra Kumar Andreas G.},
	year = {1996},
	file = {Citeseer - Full Text PDF:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\ZFI863ED\\Kumar et al. - 1996 - A Generalization Of Linear Discriminant Analysis I.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\TTXE5AZ5\\summary.html:text/html}
}

@article{li_sliced_1991,
	title = {Sliced inverse regression for dimension reduction},
	volume = {86},
	issn = {0162-1459},
	doi = {10.2307/2290563},
	abstract = {Modern advances in computing power have greatly widened scientists' scope in gathering and investigating information from many variables, information which might have been ignored in the past. Yet to effectively scan a large pool of variables is not an easy task, although our ability to interact with data has been much enhanced by recent innovations in dynamic graphics. In this article, we propose a novel data-analytic tool, sliced inverse regression (SIR), for reducing the dimension of the input variable x without going through any parametric or nonparametric model-fitting process. This method explores the simplicity of the inverse view of regression; that is, instead of regressing the univariate output variable y against the multivariate x, we regress x against y. Forward regression and inverse regression are connected by a theorem that motivates this method. The theoretical properties of SIR are investigated under a model of the form, y = f(β1x, ..., βKx, ε), where the βk's are the unknown row vectors. This model looks like a nonlinear regression, except for the crucial difference that the functional form of f is completely unknown. For effectively reducing the dimension, we need only to estimate the space [effective dimension reduction (e.d.r.) space] generated by the βk's. This makes our goal different from the usual one in regression analysis, the estimation of all the regression coefficients. In fact, the βk's themselves are not identifiable without a specific structural form on f. Our main theorem shows that under a suitable condition, if the distribution of x has been standardized to have the zero mean and the identity covariance, the inverse regression curve, E(x ∣ y), will fall into the e.d.r. space. Hence a principal component analysis on the covariance matrix for the estimated inverse regression curve can be conducted to locate its main orientation, yielding our estimates for e.d.r. directions. Furthermore, we use a simple step function to estimate the inverse regression curve. No complicated smoothing is needed. SIR can be easily implemented on personal computers. By simulation, we demonstrate how SIR can effectively reduce the dimension of the input variable from, say, 10 to K = 2 for a data set with 400 observations. The spin-plot of y against the two projected variables obtained by SIR is found to mimic the spin-plot of y against the true directions very well. A chi-squared statistic is proposed to address the issue of whether or not a direction found by SIR is spurious.},
	number = {414},
	urldate = {2016-06-22},
	journal = {J. Am. Stat. Assoc.},
	author = {Li, Ker-Chau},
	year = {1991},
	pages = {316--327}
}

@article{loog_linear_2004,
	title = {Linear dimensionality reduction via a heteroscedastic extension of {LDA}: the {Chernoff} criterion},
	volume = {26},
	issn = {0162-8828},
	shorttitle = {Linear dimensionality reduction via a heteroscedastic extension of {LDA}},
	doi = {10.1109/TPAMI.2004.13},
	abstract = {We propose an eigenvector-based heteroscedastic linear dimension reduction (LDR) technique for multiclass data. The technique is based on a heteroscedastic two-class technique which utilizes the so-called Chernoff criterion, and successfully extends the well-known linear discriminant analysis (LDA). The latter, which is based on the Fisher criterion, is incapable of dealing with heteroscedastic data in a proper way. For the two-class case, the between-class scatter is generalized so to capture differences in (co)variances. It is shown that the classical notion of between-class scatter can be associated with Euclidean distances between class means. From this viewpoint, the between-class scatter is generalized by employing the Chernoff distance measure, leading to our proposed heteroscedastic measure. Finally, using the results from the two-class case, a multiclass extension of the Chernoff criterion is proposed. This criterion combines separation information present in the class mean as well as the class covariance matrices. Extensive experiments and a comparison with similar dimension reduction techniques are presented.},
	language = {eng},
	number = {6},
	journal = {IEEE Trans. Pattern. Anal. Mach. Intell.},
	author = {Loog, Marco and Duin, Robert P. W.},
	month = jun,
	year = {2004},
	pmid = {18579934},
	keywords = {Algorithms, Artificial Intelligence, Computer Simulation, Information Storage and Retrieval, Linear Models, Pattern Recognition, Automated, Reproducibility of Results, Sensitivity and Specificity},
	pages = {732--739}
}

@article{mahanta_heteroscedastic_2012,
	title = {Heteroscedastic linear feature extraction based on sufficiency conditions},
	volume = {45},
	issn = {0031-3203},
	doi = {10.1016/j.patcog.2011.07.024},
	abstract = {Classification of high-dimensional data typically requires extraction of discriminant features. This paper proposes a linear feature extractor, called whitened linear sufficient statistic (WLSS),...},
	number = {2},
	urldate = {2016-06-22},
	journal = {Pattern Recogn.},
	author = {Mahanta, Mohammad Shahin and Aghaei, Amirhossein S. and Plataniotis, Konstantinos N. and Pasupathy, Subbarayan},
	month = feb,
	year = {2012},
	pages = {821--830},
	file = {Full Text PDF:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\8F5ICU8P\\Mahanta et al. - 2012 - Heteroscedastic linear feature extraction based on.pdf:application/pdf;Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\H2WWHRC2\\220601625_Heteroscedastic_linear_feature_extraction_based_on_sufficiency_conditions.html:text/html}
}

@article{nguyen_multi-class_2002,
	title = {Multi-class cancer classification via partial least squares with gene expression profiles},
	volume = {18},
	issn = {1367-4803},
	abstract = {MOTIVATION: Discrimination between two classes such as normal and cancer samples and between two types of cancers based on gene expression profiles is an important problem which has practical implications as well as the potential to further our understanding of gene expression of various cancer cells. Classification or discrimination of more than two groups or classes (multi-class) is also needed. The need for multi-class discrimination methodologies is apparent in many microarray experiments where various cancer types are considered simultaneously.
RESULTS: Thus, in this paper we present the extension to the classification methodology proposed earlier Nguyen and Rocke (2002b; Bioinformatics, 18, 39-50) to classify cancer samples from multiple classes. The methodologies proposed in this paper are applied to four gene expression data sets with multiple classes: (a) a hereditary breast cancer data set with (1) BRCA1-mutation, (2) BRCA2-mutation and (3) sporadic breast cancer samples, (b) an acute leukemia data set with (1) acute myeloid leukemia (AML), (2) T-cell acute lymphoblastic leukemia (T-ALL) and (3) B-cell acute lymphoblastic leukemia (B-ALL) samples, (c) a lymphoma data set with (1) diffuse large B-cell lymphoma (DLBCL), (2) B-cell chronic lymphocytic leukemia (BCLL) and (3) follicular lymphoma (FL) samples, and (d) the NCI60 data set with cell lines derived from cancers of various sites of origin. In addition, we evaluated the classification algorithms and examined the variability of the error rates using simulations based on randomization of the real data sets. We note that there are other methods for addressing multi-class prediction recently and our approach is along the line of Nguyen and Rocke (2002b; Bioinformatics, 18, 39-50).
CONTACT: dnguyen@stat.tamu.edu; dmrocke@ucdavis.edu},
	language = {eng},
	number = {9},
	journal = {Bioinformatics},
	author = {Nguyen, Danh V. and Rocke, David M.},
	month = sep,
	year = {2002},
	pmid = {12217913},
	keywords = {Algorithms, Breast Neoplasms, Databases, Nucleic Acid, Discriminant Analysis, Gene Expression Profiling, Gene Expression Regulation, Neoplastic, Humans, Least-Squares Analysis, Leukemia, Lymphoma, Models, Genetic, Models, Statistical, Neoplasms, Oligonucleotide Array Sequence Analysis, Principal Component Analysis, Reproducibility of Results, Sensitivity and Specificity, Sequence Analysis, DNA, Tumor Cells, Cultured},
	pages = {1216--1226}
}

@article{rao_utilization_1948,
	title = {The {Utilization} of {Multiple} {Measurements} in {Problems} of {Biological} {Classification}},
	volume = {10},
	issn = {0035-9246},
	number = {2},
	urldate = {2016-06-22},
	journal = {J. Roy. Stat. Soc. B Met.},
	author = {Rao, C. Radhakrishna},
	year = {1948},
	pages = {159--203}
}

@article{tubbs_linear_1982,
	title = {Linear dimension reduction and {Bayes} classification with unknown population parameters},
	volume = {15},
	abstract = {Odell and Decell, Odell and Coberly gave necessary and sufficient conditions for the smallest dimension compression matrix B such that the Bayes classification regions are preserved. That is, they developed an explicit expression of a compression matrix B such that the Bayes classification assignment are the same for both the original space x and the compressed space Bx. Odell indicated that whenever the population parameters are unknown, then the dimension of Bx is the same as x with probability one. Furthermore, Odell posed the problem of finding a lower dimension q {\textless} p which in some sense best fits the range space generated by the matrix M. The purpose of this paper is to discuss this problem and provide a partial solution. © 1982.},
	language = {English},
	number = {3},
	journal = {Pattern Recogn.},
	author = {Tubbs, J. D. and Coberly, W. A. and Young, D. M.},
	year = {1982},
	keywords = {Bayes classification procedure, Dimension reduction, Feature selection, Probability of misclassification, Projection operator, Singular value decomposition},
	pages = {167--172}
}

@article{velilla_method_2008,
	title = {A method for dimension reduction in quadratic classification problems},
	volume = {17},
	issn = {1061-8600},
	abstract = {This article presents a dimension-reduction method in quadratic discriminant analysis (QDA). The procedure is inspired by the geometric relation that exists between the subspaces used in sliced inverse regression (SIR) and sliced average variance estimation (SAVE). A new set of directions is constructed to improve the properties of the directions associated with the eigenvectors of the matrices usually considered for dimension reduction in QDA. Illustrative examples of application with real and simulated data are discussed.},
	number = {3},
	urldate = {2016-06-22},
	journal = {J. Comput. Graph. Stat.},
	author = {Velilla, Santiago},
	year = {2008},
	pages = {572--589}
}

@article{velilla_consistency_2005,
	title = {On the consistency properties of linear and quadratic discriminant analyses},
	volume = {96},
	issn = {0047-259X},
	doi = {10.1016/j.jmva.2004.10.009},
	abstract = {The limit behavior of the conditional probability of error of linear and quadratic discriminant analyses is studied under wide assumptions on the class conditional distributions. Results obtained may help to explain analytically the behavior in applications of linear and quadratic discrimination techniques.},
	number = {2},
	urldate = {2016-06-22},
	journal = {J. Multivariate Anal.},
	author = {Velilla, Santiago and Hernandez, Adolfo},
	month = oct,
	year = {2005},
	keywords = {Bayes error, Conditional probability of misclassification, Consistent sample discriminant rules, Inverse regression models, Plug-in sample discriminant rules},
	pages = {219--236},
	file = {ScienceDirect Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\XNSPA7ZM\\S0047259X04002179.html:text/html}
}

@article{young_quadratic_1987,
	title = {Quadratic discrimination: {Some} results on optimal low-dimensional representation},
	volume = {17},
	issn = {0378-3758},
	shorttitle = {Quadratic discrimination},
	doi = {10.1016/0378-3758(87)90122-4},
	abstract = {A random vector is assumed to belong to one several multivariate normal distributions possibility having unequal covariance matrices. The goal is to find a low-dimensional hyperplane which preserves or nearly preserves the separation of the individual population. We present a computationally simple method of deriving a linear transformation for low-dimensional representation and give conditions under which the Bayes classification rule is preserved in the low-dimensional space. Finally, we give several examples to demonstrate the method.},
	urldate = {2016-06-23},
	journal = {J. Stat. Plan. Infer.},
	author = {Young, Dean M. and Marco, Virgil R. and Odell, Patrick L.},
	month = jan,
	year = {1987},
	keywords = {Bayes classification, Linear transformation, Probability of misclassification, Quadratic discrimination},
	pages = {307--319},
	file = {ScienceDirect Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\TBFS7HV4\\0378375887901224.html:text/html}
}

@article{hartwig_reverse_1986,
	title = {The reverse order law revisited},
	volume = {76},
	issn = {0024-3795},
	doi = {10.1016/0024-3795(86)90226-0},
	abstract = {Necessary and sufficient conditions are given for the triple reverse order law (ABC)† = C†B†A† to hold. Some special cases are considered.},
	urldate = {2016-07-06},
	journal = {Linear. Algebra. Appl.},
	author = {Hartwig, R. E.},
	month = apr,
	year = {1986},
	pages = {241--246},
	file = {ScienceDirect Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\37Q52RI5\\0024379586902260.html:text/html}
}

@book{rao_linear_1973,
	address = {New York},
	series = {Wiley series in probability and mathematical statistics},
	title = {Linear statistical inference and its applications},
	isbn = {978-0-471-70823-0},
	urldate = {2016-07-07},
	publisher = {Wiley},
	author = {Rao, C. Radhakrishna},
	year = {1973},
	keywords = {Mathematical statistics},
	file = {Hathi Trust Record:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\VT6EBMVW\\000007079.html:text/html}
}

@article{eckart_approximation_1936,
	title = {The approximation of one matrix by another of lower rank},
	volume = {1},
	issn = {0033-3123, 1860-0980},
	doi = {10.1007/BF02288367},
	abstract = {The mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique. Several conclusions can be drawn from the form of this solution. A hypothetical interpretation of the canonic components of a score matrix is discussed.},
	language = {en},
	number = {3},
	urldate = {2016-07-12},
	journal = {Psychometrika},
	author = {Eckart, Carl and Young, Gale},
	year = {1936},
	pages = {211--218},
	file = {Full Text PDF:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\D8VM55XM\\Eckart and Young - The approximation of one matrix by another of lowe.pdf:application/pdf;Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\6CIVDI5G\\BF02288367.html:text/html}
}

@article{sigillito_classification_1989,
	title = {Classification of radar returns from the ionosphere using neural networks},
	volume = {vol. 10},
	journal = {J. Hopkins Apl. Tech. D.},
	author = {Sigillito, V. G. and Wing, S. P. and Hutton, L. V. and Baker, K. B.},
	year = {1989},
	note = {in},
	pages = {262--266}
}

@book{bache_uci_2013,
	title = {{UCI} {Machine} {Learning} {Repository}},
	publisher = {University of California, Irvine, School of Information and Computer Sciences},
	author = {Bache, Kevin and Lichman, Moshe},
	year = {2013}
}

@article{fan_quadro:_2015,
	title = {{QUADRO}: {A} supervised dimension reduction method via {Rayleigh} quotient optimization},
	volume = {43},
	issn = {0090-5364},
	shorttitle = {{QUADRO}},
	doi = {10.1214/14-AOS1307},
	abstract = {We propose a novel Rayleigh quotient based sparse quadratic dimension reduction method - named QUADRO (Quadratic Dimension Reduction via Rayleigh Optimization) - for analyzing high- dimensional data. Unlike in the linear setting where Rayleigh quotient optimization coincides with classification, these two problems are very different under nonlinear settings. In this paper, we clarify this difference and show that Rayleigh quotient optimization may be of independent scientific interests. One major challenge of Rayleigh quotient optimization is that the variance of quadratic statistics involves all fourth cross-moments of predictors, which are infeasible to compute for high-dimensional applications and may accumulate too many stochastic errors. This issue is resolved by considering a family of elliptical models. Moreover, for heavy-tail distributions, robust estimates of mean vectors and covariance matrices are employed to guarantee uniform convergence in estimating nonpolynomially many parameters, even though only the fourth moments are assumed. Methodologically, QUADRO is based on elliptical models which allow us to formulate the Rayleigh quotient maximization as a convex optimization problem. Computationally, we propose an efficient linearized augmented Lagrangian method to solve the constrained optimization problem. Theoretically, we provide explicit rates of convergence in terms of Rayleigh quotient under both Gaussian and general elliptical models. Thorough numerical results on both synthetic and real datasets are also provided to back up our theoretical results.},
	number = {4},
	urldate = {2016-09-15},
	journal = {Ann. Stat.},
	author = {Fan, Jianqing and Ke, Zheng Tracy and Liu, Han and Xia, Lucy},
	month = aug,
	year = {2015},
	keywords = {Statistics - Methodology},
	pages = {1498--1534},
	file = {arXiv\:1311.5542 PDF:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\S59SQN3T\\Fan et al. - 2015 - QUADRO A supervised dimension reduction method vi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\7S4CR2FW\\1311.html:text/html}
}

@article{peck_comparison_1988,
	title = {A comparison of several biased estimators for improving the expected error rate of the sample quadratic discriminant function},
	volume = {29},
	issn = {0094-9655},
	doi = {10.1080/00949658808811057},
	abstract = {The sample quadratic discriminant function (QDF) has been shown by Marks and Dunn (1974) to be superior to the linear discriminant function for two normal populations with , provided the training sample sizesn 1and n 2, are sufficiently large. However, the performance of the QDF quickly deteriorates as the dimension p increases relative to the sample size n i i = l , 2 . The deterioration is principally due to poor estimates of the inverse of the covariance matrices, . One method of combating this problem is to apply biased estimators of the inverse of the covariance matrices. In this paper we contrast the performance of the QDF with respect to several biased estimators and one unbiased estimator of A shrinkage estimator proposed by Peck and Van Ness (1982) is found to yield superior performance over a wide range of configurations and training sample sizes.},
	number = {2},
	urldate = {2016-09-15},
	journal = {J. Stat. Comput. Sim.},
	author = {Peck, Roger and Jennings, Linda W. and Young, Dean M.},
	month = apr,
	year = {1988},
	pages = {143--156},
	file = {Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\KZ849C6Z\\00949658808811057.html:text/html}
}

@article{schott_determining_1994,
	title = {Determining the dimensionality in sliced inverse regression},
	volume = {89},
	issn = {0162-1459},
	doi = {10.2307/2291210},
	abstract = {A general regression problem is one in which a response variable can be expressed as some function of one or more different linear combinations of a set of explanatory variables as well as a random error term. Sliced inverse regression is a method for determining these linear combinations. In this article we address the problem of determining how many linear combinations are involved. Procedures based on conditional means and conditional covariance matrices, as well as a procedure combining the two approaches, are considered. In each case we develop a test that has an asymptotic chi-squared distribution when the vector of explanatory variables is sampled from an elliptically symmetric distribution.},
	number = {425},
	urldate = {2016-09-27},
	journal = {J. Am. Stat. Assoc.},
	author = {Schott, James R.},
	year = {1994},
	pages = {141--148}
}

@article{cook_envelope_2010,
	title = {Envelope models for parsimonious and efficient multivariate linear regression},
	volume = {20},
	issn = {1017-0405},
	abstract = {We propose a new parsimonious version of the classical multivariate normal linear model, yielding a maximum likelihood estimator (MLE) that is asymptotically less variable than the MLE based on the usual model. Our approach is based on the construction of a link between the mean function and the covariance matrix, using the minimal reducing subspace of the latter that accommodates the former. This leads to a multivariate regression model that we call the envelope model, where the number of parameters is maximally reduced. The MLE from the envelope model can be substantially less variable than the usual MLE, especially when the mean function varies in directions that are orthogonal to the directions of maximum variation for the covariance matrix.},
	number = {3},
	urldate = {2016-09-27},
	journal = {Stat. Sinica},
	author = {Cook, R. Dennis and Li, Bing and Chiaromonte, Francesca},
	year = {2010},
	pages = {927--960}
}

@article{cook_covariance_2008,
	title = {Covariance reducing models: {An} alternative to spectral modelling of covariance matrices},
	volume = {95},
	issn = {0006-3444, 1464-3510},
	shorttitle = {Covariance reducing models},
	doi = {10.1093/biomet/asn052},
	abstract = {We introduce covariance reducing models for studying the sample covariance matrices of a random vector observed in different populations. The models are based on reducing the sample covariance matrices to an informational core that is sufficient to characterize the variance heterogeneity among the populations. They possess useful equivariance properties and provide a clear alternative to spectral models for covariance matrices.},
	language = {en},
	number = {4},
	urldate = {2016-09-28},
	journal = {Biometrika},
	author = {Cook, R. Dennis and Forzani, Liliana},
	month = dec,
	year = {2008},
	keywords = {Central subspace, dimension reduction, Envelopes, Grassmann manifolds, Reducing subspaces},
	pages = {799--812},
	file = {Full Text PDF:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\37JWFZQN\\Cook and Forzani - 2008 - Covariance reducing models An alternative to spec.pdf:application/pdf;Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\3CG6SMA9\\799.html:text/html}
}

@article{peters_characterizations_1978,
	title = {Characterizations of {Linear} {Sufficient} {Statistics}},
	volume = {40},
	issn = {0581-572X},
	abstract = {We develop necessary and sufficient conditions that a surjective bounded linear operator T from a linear space X to a linear space Y be a sufficient statistic for a dominated family of probability measures defined on the Borel sets of X. We give applications of these results that characterize linear sufficient statistics for families of the exponential type, including as special cases the Wishart and multivariate normal distributions.},
	number = {3},
	urldate = {2016-11-10},
	journal = {Sankhya Ser. A},
	author = {Peters, B. Charles and Redner, Richard and Decell, Henry P.},
	year = {1978},
	pages = {303--309}
}

@article{fischer_distributionfree_1979,
	title = {On a distributionfree method in discriminant analysis},
	volume = {10},
	issn = {0323-3944},
	doi = {10.1080/02331887908801484},
	abstract = {Linear discriminant rules for two symmetrical distributions, which only need the first and second moments of these distributions, are presented. The rules are based on Zhezhel's idea using the most unfavourable probabilities of misclassification as an optimality criterion. Also a rule is considered which deals with distributions differing in a location and scale parameter.},
	number = {2},
	urldate = {2016-12-19},
	journal = {Statistics},
	author = {Fischer, K. and Thiele, Chr},
	month = jan,
	year = {1979},
	keywords = {BAYESian rule, Discriminant Analysis, minimax rule, most unfavourable probability of misclassification},
	pages = {281--289},
	file = {Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\NSQVEBGU\\02331887908801484.html:text/html}
}

@article{pavlenko_effect_2001,
	title = {Effect of dimensionality on discrimination},
	volume = {35},
	issn = {0233-1888},
	doi = {10.1080/02331880108802731},
	abstract = {Discrimination problems in a high-dimensional setting is considered. New results are concerned with the role of the dimensionality in the performance of the discrimination procedure. Assuming that data consist of a block structure two different asymptotic approaches are presented. These approaches are characterized by different types of relations between the dimensionality and the size of the training samples. Asymptotic expressions for the error probabilities are obtained and a consistent approximation of the discriminant function is proposed. Throughout the paper the importance of the dimensionality in the asymptotic analysis is stressed.},
	number = {3},
	urldate = {2016-12-21},
	journal = {Statistics},
	author = {Pavlenko, Tatjana and Rosen, Dietrich Von},
	month = jan,
	year = {2001},
	keywords = {Discriminant Analysis, Growing dimension asymptotic, Likelihood based discrimination, Limiting error probability, Plug-in estimator},
	pages = {191--213},
	file = {Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\R9XSMVBT\\02331880108802731.html:text/html}
}