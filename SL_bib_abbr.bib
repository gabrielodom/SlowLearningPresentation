
@article{ounpraseuth_linear_2015,
	title = {Linear dimension reduction for multiple heteroscedastic multivariate normal populations},
	volume = {05},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	doi = {10.4236/ojs.2015.54033},
	abstract = {For the case where all multivariate normal parameters are known, we derive a new linear dimension reduction (LDR) method to determine a low-dimensional subspace that preserves or nearly preserves the original feature-space separation of the individual populations and the Bayes probability of misclassification. We also give necessary and sufficient conditions which provide the smallest reduced dimension that essentially retains the Bayes probability of misclassification from the original full-dimensional space in the reduced space. Moreover, our new LDR procedure requires no computationally expensive optimization procedure. Finally, for the case where parameters are unknown, we devise a LDR method based on our new theorem and compare our LDR method with three competing LDR methods using Monte Carlo simulations and a parametric bootstrap based on real data.},
	language = {en},
	number = {04},
	urldate = {2016-06-22},
	journal = {OJS},
	author = {Ounpraseuth, Songthip T. and Young, Phil D. and Van Zyl, Johanna S. and Nelson, Tyler W. and Young, Dean M.},
	month = may,
	year = {2015},
	pages = {311},
	file = {Full Text PDF:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\TH8XQ62W\\Ounpraseuth et al. - 2015 - Linear Dimension Reduction for Multiple Heterosced.pdf:application/pdf;Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\F9DG9S95\\PaperInformation.html:text/html}
}

@book{bellman_adaptive_1961,
	title = {Adaptive {Control} {Processes}: {A} {Guided} {Tour}},
	isbn = {978-0-691-07901-1},
	shorttitle = {Adaptive {Control} {Processes}},
	abstract = {The book description for "Adaptive Control Processes" is currently unavailable.},
	urldate = {2016-06-22},
	publisher = {Princeton University Press},
	author = {Bellman, Richard},
	year = {1961}
}

@article{cook_sliced_1991,
	title = {Sliced inverse regression for dimension reduction: comment},
	volume = {86},
	issn = {0162-1459},
	shorttitle = {Sliced {Inverse} {Regression} for {Dimension} {Reduction}},
	doi = {10.2307/2290564},
	number = {414},
	urldate = {2016-06-22},
	journal = {J. Am. Stat. Assoc.},
	author = {Cook, R. Dennis and Weisberg, Sanford},
	year = {1991},
	pages = {328--332},
	file = {JSTOR Full Text PDF:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\V8BRVCMI\\Cook and Weisberg - 1991 - Sliced Inverse Regression for Dimension Reduction.pdf:application/pdf}
}

@article{cook_theory_2001,
	title = {Theory \& methods: {Special} {Invited} {Paper}: {Dimension} reduction and visualization in discriminant analysis},
	volume = {43},
	copyright = {Australian Statistical Publishing Association Inc 2001},
	issn = {1467-842X},
	shorttitle = {Theory \& {Methods}},
	doi = {10.1111/1467-842X.00164},
	abstract = {This paper discusses visualization methods for discriminant analysis. It does not address numerical methods for classification per se, but rather focuses on graphical methods that can be viewed as pre-processors, aiding the analyst's understanding of the data and the choice of a final classifier. The methods are adaptations of recent results in dimension reduction for regression, including sliced inverse regression and sliced average variance estimation. A permutation test is suggested as a means of determining dimension, and examples are given throughout the discussion.},
	language = {en},
	number = {2},
	urldate = {2016-06-22},
	journal = {Aust. N.Z. J. Stat.},
	author = {Cook, R. Dennis and Yin, Xiangrong},
	month = jun,
	year = {2001},
	keywords = {central subspaces, dimension reduction, regression, regression graphics, sliced average variance estimation (SAVE)., sliced inverse regression (SIR)},
	pages = {147--199},
	file = {Full Text PDF:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\ZR8AI58W\\Cook and Yin - 2001 - Theory & Methods Special Invited Paper Dimension.pdf:application/pdf;Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\GFF7FGU3\\abstract.html:text/html}
}

@article{haff_estimation_1979,
	title = {Estimation of the inverse covariance matrix: random mixtures of the inverse wishart matrix and the identity},
	volume = {7},
	issn = {0090-5364},
	shorttitle = {Estimation of the {Inverse} {Covariance} {Matrix}},
	abstract = {Let Sp × p have a nonsingular Wishart distribution with unknown matrix Σ and k degrees of freedom. For two different loss functions, estimators of Σ-1 are given which dominate the obvious estimators \$aS{\textasciicircum}\{-1\}, 0 {\textless} a {\textbackslash}leqslant k - p - 1\$. Our class of estimators C includes random mixtures of S-1 and I. A subclass \${\textbackslash}mathscr\{C\}\_0 {\textbackslash}subset {\textbackslash}mathscr\{C\}\$ was given by Haff. Here, we show that any member of C0 is dominated in C. Some troublesome aspects of the estimation problem are discussed, and the theory is supplemented by simulation results.},
	number = {6},
	urldate = {2016-06-22},
	journal = {Ann. Stat.},
	author = {Haff, L. R.},
	year = {1979},
	pages = {1264--1276}
}

@article{li_sliced_1991,
	title = {Sliced inverse regression for dimension reduction},
	volume = {86},
	issn = {0162-1459},
	doi = {10.2307/2290563},
	abstract = {Modern advances in computing power have greatly widened scientists' scope in gathering and investigating information from many variables, information which might have been ignored in the past. Yet to effectively scan a large pool of variables is not an easy task, although our ability to interact with data has been much enhanced by recent innovations in dynamic graphics. In this article, we propose a novel data-analytic tool, sliced inverse regression (SIR), for reducing the dimension of the input variable x without going through any parametric or nonparametric model-fitting process. This method explores the simplicity of the inverse view of regression; that is, instead of regressing the univariate output variable y against the multivariate x, we regress x against y. Forward regression and inverse regression are connected by a theorem that motivates this method. The theoretical properties of SIR are investigated under a model of the form, y = f(β1x, ..., βKx, ε), where the βk's are the unknown row vectors. This model looks like a nonlinear regression, except for the crucial difference that the functional form of f is completely unknown. For effectively reducing the dimension, we need only to estimate the space [effective dimension reduction (e.d.r.) space] generated by the βk's. This makes our goal different from the usual one in regression analysis, the estimation of all the regression coefficients. In fact, the βk's themselves are not identifiable without a specific structural form on f. Our main theorem shows that under a suitable condition, if the distribution of x has been standardized to have the zero mean and the identity covariance, the inverse regression curve, E(x ∣ y), will fall into the e.d.r. space. Hence a principal component analysis on the covariance matrix for the estimated inverse regression curve can be conducted to locate its main orientation, yielding our estimates for e.d.r. directions. Furthermore, we use a simple step function to estimate the inverse regression curve. No complicated smoothing is needed. SIR can be easily implemented on personal computers. By simulation, we demonstrate how SIR can effectively reduce the dimension of the input variable from, say, 10 to K = 2 for a data set with 400 observations. The spin-plot of y against the two projected variables obtained by SIR is found to mimic the spin-plot of y against the true directions very well. A chi-squared statistic is proposed to address the issue of whether or not a direction found by SIR is spurious.},
	number = {414},
	urldate = {2016-06-22},
	journal = {J. Am. Stat. Assoc.},
	author = {Li, Ker-Chau},
	year = {1991},
	pages = {316--327}
}

@article{loog_linear_2004,
	title = {Linear dimensionality reduction via a heteroscedastic extension of {LDA}: the {Chernoff} criterion},
	volume = {26},
	issn = {0162-8828},
	shorttitle = {Linear dimensionality reduction via a heteroscedastic extension of {LDA}},
	doi = {10.1109/TPAMI.2004.13},
	abstract = {We propose an eigenvector-based heteroscedastic linear dimension reduction (LDR) technique for multiclass data. The technique is based on a heteroscedastic two-class technique which utilizes the so-called Chernoff criterion, and successfully extends the well-known linear discriminant analysis (LDA). The latter, which is based on the Fisher criterion, is incapable of dealing with heteroscedastic data in a proper way. For the two-class case, the between-class scatter is generalized so to capture differences in (co)variances. It is shown that the classical notion of between-class scatter can be associated with Euclidean distances between class means. From this viewpoint, the between-class scatter is generalized by employing the Chernoff distance measure, leading to our proposed heteroscedastic measure. Finally, using the results from the two-class case, a multiclass extension of the Chernoff criterion is proposed. This criterion combines separation information present in the class mean as well as the class covariance matrices. Extensive experiments and a comparison with similar dimension reduction techniques are presented.},
	language = {eng},
	number = {6},
	journal = {IEEE Trans. Pattern. Anal. Mach. Intell.},
	author = {Loog, Marco and Duin, Robert P. W.},
	month = jun,
	year = {2004},
	pmid = {18579934},
	keywords = {Algorithms, Artificial Intelligence, Computer Simulation, Information Storage and Retrieval, Linear Models, Pattern Recognition, Automated, Reproducibility of Results, Sensitivity and Specificity},
	pages = {732--739}
}

@article{tubbs_linear_1982,
	title = {Linear dimension reduction and {Bayes} classification with unknown population parameters},
	volume = {15},
	abstract = {Odell and Decell, Odell and Coberly gave necessary and sufficient conditions for the smallest dimension compression matrix B such that the Bayes classification regions are preserved. That is, they developed an explicit expression of a compression matrix B such that the Bayes classification assignment are the same for both the original space x and the compressed space Bx. Odell indicated that whenever the population parameters are unknown, then the dimension of Bx is the same as x with probability one. Furthermore, Odell posed the problem of finding a lower dimension q {\textless} p which in some sense best fits the range space generated by the matrix M. The purpose of this paper is to discuss this problem and provide a partial solution. © 1982.},
	language = {English},
	number = {3},
	journal = {Pattern Recogn.},
	author = {Tubbs, J. D. and Coberly, W. A. and Young, D. M.},
	year = {1982},
	keywords = {Bayes classification procedure, Dimension reduction, Feature selection, Probability of misclassification, Projection operator, Singular value decomposition},
	pages = {167--172}
}

@article{velilla_method_2008,
	title = {A method for dimension reduction in quadratic classification problems},
	volume = {17},
	issn = {1061-8600},
	abstract = {This article presents a dimension-reduction method in quadratic discriminant analysis (QDA). The procedure is inspired by the geometric relation that exists between the subspaces used in sliced inverse regression (SIR) and sliced average variance estimation (SAVE). A new set of directions is constructed to improve the properties of the directions associated with the eigenvectors of the matrices usually considered for dimension reduction in QDA. Illustrative examples of application with real and simulated data are discussed.},
	number = {3},
	urldate = {2016-06-22},
	journal = {J. Comput. Graph. Stat.},
	author = {Velilla, Santiago},
	year = {2008},
	pages = {572--589}
}

@article{velilla_consistency_2005,
	title = {On the consistency properties of linear and quadratic discriminant analyses},
	volume = {96},
	issn = {0047-259X},
	doi = {10.1016/j.jmva.2004.10.009},
	abstract = {The limit behavior of the conditional probability of error of linear and quadratic discriminant analyses is studied under wide assumptions on the class conditional distributions. Results obtained may help to explain analytically the behavior in applications of linear and quadratic discrimination techniques.},
	number = {2},
	urldate = {2016-06-22},
	journal = {J. Multivariate Anal.},
	author = {Velilla, Santiago and Hernandez, Adolfo},
	month = oct,
	year = {2005},
	keywords = {Bayes error, Conditional probability of misclassification, Consistent sample discriminant rules, Inverse regression models, Plug-in sample discriminant rules},
	pages = {219--236},
	file = {ScienceDirect Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\XNSPA7ZM\\S0047259X04002179.html:text/html}
}

@article{eckart_approximation_1936,
	title = {The approximation of one matrix by another of lower rank},
	volume = {1},
	issn = {0033-3123, 1860-0980},
	doi = {10.1007/BF02288367},
	abstract = {The mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique. Several conclusions can be drawn from the form of this solution. A hypothetical interpretation of the canonic components of a score matrix is discussed.},
	language = {en},
	number = {3},
	urldate = {2016-07-12},
	journal = {Psychometrika},
	author = {Eckart, Carl and Young, Gale},
	year = {1936},
	pages = {211--218},
	file = {Full Text PDF:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\D8VM55XM\\Eckart and Young - The approximation of one matrix by another of lowe.pdf:application/pdf;Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\6CIVDI5G\\BF02288367.html:text/html}
}

@article{sigillito_classification_1989,
	title = {Classification of radar returns from the ionosphere using neural networks},
	volume = {vol. 10},
	journal = {J. Hopkins Apl. Tech. D.},
	author = {Sigillito, V. G. and Wing, S. P. and Hutton, L. V. and Baker, K. B.},
	year = {1989},
	note = {in},
	pages = {262--266}
}

@book{bache_uci_2013,
	title = {{UCI} {Machine} {Learning} {Repository}},
	publisher = {University of California, Irvine, School of Information and Computer Sciences},
	author = {Bache, Kevin and Lichman, Moshe},
	year = {2013}
}

@book{gutman_mathematical_1986,
	address = {Berlin, Heidelberg},
	title = {Mathematical {Concepts} in {Organic} {Chemistry}},
	isbn = {978-3-642-70984-5 978-3-642-70982-1},
	language = {en},
	urldate = {2016-08-01},
	publisher = {Springer Berlin Heidelberg},
	author = {Gutman, Ivan and Polansky, Oskar E.},
	year = {1986}
}

@inproceedings{paschke_sensorlose_2013,
	title = {Sensorlose {Zustandsüberwachung} an {Synchronmotoren}},
	abstract = {Official Full-Text Publication: Sensorlose Zustandsüberwachung an Synchronmotoren on ResearchGate, the professional network for scientists.},
	urldate = {2017-01-17},
	booktitle = {{ResearchGate}},
	author = {Paschke, Fabian and Bayer, Christian and Bator, Martyna and Mönks, Uwe and Dicks, Alexander and Enge-Rosenblatt, Olaf and Lohweg, Volker},
	month = dec,
	year = {2013},
	file = {Full Text PDF:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\V6SW78M8\\Paschke et al. - 2013 - Sensorlose Zustandsüberwachung an Synchronmotoren.pdf:application/pdf;Snapshot:C\:\\Users\\gabriel_odom\\AppData\\Roaming\\Mozilla\\Firefox\\Profiles\\0t84j5fh.default\\zotero\\storage\\FGZ2Q47R\\262698433_Sensorlose_Zustandsuberwachung_an_Synchronmotoren.html:text/html}
}